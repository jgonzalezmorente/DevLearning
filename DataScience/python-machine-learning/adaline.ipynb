{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12e159d-09ef-49c0-8889-6a50e882f04b",
   "metadata": {},
   "source": [
    "# Neuronas Lineales Adaptativas (ADALINE)\n",
    "\n",
    "El algoritmo **Adaline** (Adaptive Linear Neuron) es particularmente interesante porque ilustra conceptos fundamentales en el aprendizaje automático, como la definición y minimización de funciones de coste continuas. Estos conceptos son esenciales para entender algoritmos más avanzados de clasificación, como la **regresión logística**, las **máquinas de vectores de soporte** (SVM) y los modelos de regresión.\n",
    "\n",
    "## Diferencias entre Adaline y el Perceptrón\n",
    "\n",
    "Una diferencia clave entre **Adaline** y el **perceptrón de Rosenblatt** radica en cómo se actualizan los pesos. Mientras que el perceptrón utiliza una función de activación basada en un **escalón unitario**, Adaline utiliza una **función de activación lineal**. En Adaline, la actualización de los pesos se realiza después de evaluar todo el conjunto de datos, mientras que en el perceptrón, los pesos se actualizan tras cada muestra de entrenamiento. Este enfoque en Adaline se conoce como **descenso de gradiente en lotes**, ya que se ajustan los pesos considerando todas las muestras de una vez.\n",
    "\n",
    "En Adaline, la función de activación lineal, representada por $\\phi(z)$, es simplemente el producto escalar de los pesos y las entradas de la red:\n",
    "$$\n",
    "\\phi\\left(\\mathbf{w}^T\\mathbf{x}\\right) = \\mathbf{w}^T\\mathbf{x}\n",
    "$$\n",
    "Mientras que esta función de activación lineal se emplea para ajustar los pesos, el algoritmo sigue utilizando una **función umbral** para realizar la predicción final, que es similar a la función escalón del perceptrón. Es decir, aunque los pesos se ajustan basándose en valores continuos, la predicción final será una etiqueta de clase discreta (por ejemplo, +1 o -1), utilizando una función umbral que se aplica sobre la salida lineal.\n",
    "\n",
    "**Adaline** compara las etiquetas verdaderas de las clases con los valores continuos generados por la función de activación lineal para calcular el error. Esta es otra diferencia importante respecto al perceptrón, que compara directamente las etiquetas verdaderas con las etiquetas de clase predichas.\n",
    "\n",
    "## Minimizar la función de coste con el descenso del gradiente\n",
    "\n",
    "En Adaline, la función de coste se define como la **Suma de Errores Cuadráticos (SSE)** entre la salida calculada y la etiqueta de clase verdadera:\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{2} \\sum_i \\left( y^{(i)} - \\phi\\left(z^{(i)}\\right) \\right)^2\n",
    "$$\n",
    "Esta función de coste es convexa y diferenciable, lo que permite aplicar el algoritmo de **descenso del gradiente** para minimizarla. El objetivo es encontrar los pesos que minimicen esta función de coste ajustando los pesos actuales:\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}\n",
    "$$\n",
    "donde $\\Delta\\mathbf{w}$ representa el cambio en los pesos, calculado como el **gradiente negativo** multiplicado por la tasa de aprendizaje $\\eta$:\n",
    "$$\n",
    "\\Delta\\mathbf{w} = -\\eta \\nabla J(\\mathbf{w})\n",
    "$$\n",
    "El gradiente de la función de coste se obtiene calculando la derivada parcial con respecto a cada peso $w_j$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = -\\sum_i \\left( y^{(i)} - \\phi\\left(z^{(i)}\\right) \\right) x_j^{(i)}\n",
    "$$\n",
    "Esto lleva a la expresión del gradiente:\n",
    "$$\n",
    "\\nabla J(\\mathbf{w}) = - \\left( \\mathbf{y} - \\phi(\\mathbf{z}) \\right) X\n",
    "$$\n",
    "donde $X$ es la matriz de datos de entrada, cuyas filas son las muestras de entrenamiento.\n",
    "\n",
    "Finalmente, el cambio en los pesos se expresa como:\n",
    "$$\n",
    "\\Delta\\mathbf{w} = \\eta \\left( \\mathbf{y} - \\phi(\\mathbf{z}) \\right) X\n",
    "$$\n",
    "y los pesos se actualizan con la fórmula:\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} + \\eta \\left( \\mathbf{y} - \\phi(\\mathbf{z}) \\right) X\n",
    "$$\n",
    "\n",
    "Aunque superficialmente la regla de aprendizaje de Adaline puede parecer similar a la del perceptrón, la diferencia radica en que $\\phi\\left(z^{(i)}\\right)$ es un valor continuo y no una etiqueta de clase categórica. Además, la actualización de los pesos en Adaline se realiza evaluando todo el conjunto de datos, mientras que en el perceptrón, los pesos se ajustan después de procesar cada muestra de manera incremental. Esto hace que Adaline utilice el enfoque de **descenso de gradiente en lotes**, una técnica más robusta que facilita la convergencia en problemas donde la función de coste es continua y diferenciable.\n",
    "\n",
    "## Resumen\n",
    "\n",
    "En resumen, **Adaline** constituye un importante paso hacia algoritmos más sofisticados de aprendizaje automático y permite el uso de métodos optimizados como el descenso del gradiente para ajustar los parámetros de los modelos de manera eficiente. La capacidad de definir y minimizar una función de coste diferenciable hace que Adaline sea una base fundamental para comprender los algoritmos más avanzados utilizados hoy en día en clasificación y regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ad822-8faf-4976-b170-9dd7022b991c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
